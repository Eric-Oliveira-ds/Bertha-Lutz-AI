{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a949802a",
   "metadata": {},
   "source": [
    "# BERTHA LUTZ DEV AGENTIC"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6c278d1",
   "metadata": {},
   "source": [
    "- LLM responde usando RAG dos protocolos do Ministério da Saúde > Guarda histórico no PostgreSQL > Aplica guardrails (não diagnostica, não prescreve)> Usa LangGraph + tools"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6f08cbd",
   "metadata": {},
   "source": [
    "## Ingerir PDFs do MS no Chroma (RAG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "df5b233d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\erico\\Documents\\Bertha-Lutz-AI\\env\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_huggingface import HuggingFaceEmbeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "41cc0b19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# rag/ingest.py\n",
    "\n",
    "def ingest(pdf_path: str):\n",
    "    loader = PyPDFLoader(pdf_path)\n",
    "    docs = loader.load()\n",
    "\n",
    "    splitter = RecursiveCharacterTextSplitter(chunk_size=800, chunk_overlap=100)\n",
    "    chunks = splitter.split_documents(docs)\n",
    "\n",
    "    embeddings = HuggingFaceEmbeddings(model_name=\"hkunlp/instructor-base\")\n",
    "\n",
    "    vectordb = Chroma.from_documents(\n",
    "        chunks,\n",
    "        embedding=embeddings,\n",
    "        persist_directory=\"chroma_db\"\n",
    "    )\n",
    "\n",
    "    vectordb.persist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b8a26c2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ingest(r'pdf\\Consensointegra.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b1d3ff35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ingest(r'pdf\\relatorio-preliminar-diretrizes-brasileiras-para-o-rastreamento-do-cancer-do-colo-do-utero-parte-i-rastreamento-organizado-utilizando-testes-moleculares-para-deteccao-de-dna-hpv-oncogenico.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4011b225",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ingest(r'pdf\\Manual da Gestante.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e28fd7f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading weights: 100%|██████████| 100/100 [00:00<00:00, 543.72it/s, Materializing param=shared.weight]                                                     \n",
      "The tied weights mapping and config for this model specifies to tie shared.weight to encoder.embed_tokens.weight, but both are present in the checkpoints, so we will NOT tie them. You should update the config with `tie_word_embeddings=False` to silence this warning\n",
      "Warning: You are sending unauthenticated requests to the HF Hub. Please set a HF_TOKEN to enable higher rate limits and faster downloads.\n",
      "C:\\Users\\erico\\AppData\\Local\\Temp\\ipykernel_6460\\3887741603.py:18: LangChainDeprecationWarning: Since Chroma 0.4.x the manual persistence method is no longer supported as docs are automatically persisted.\n",
      "  vectordb.persist()\n"
     ]
    }
   ],
   "source": [
    "ingest(r'pdf\\femina-2019-474-241-244.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a4bdc4ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading weights: 100%|██████████| 100/100 [00:00<00:00, 522.63it/s, Materializing param=shared.weight]                                                     \n",
      "The tied weights mapping and config for this model specifies to tie shared.weight to encoder.embed_tokens.weight, but both are present in the checkpoints, so we will NOT tie them. You should update the config with `tie_word_embeddings=False` to silence this warning\n"
     ]
    }
   ],
   "source": [
    "ingest(r'pdf\\infeccoes_sexualmente_transmissiveis.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c6f86d52",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading weights: 100%|██████████| 100/100 [00:00<00:00, 375.54it/s, Materializing param=shared.weight]                                                     \n",
      "The tied weights mapping and config for this model specifies to tie shared.weight to encoder.embed_tokens.weight, but both are present in the checkpoints, so we will NOT tie them. You should update the config with `tie_word_embeddings=False` to silence this warning\n"
     ]
    }
   ],
   "source": [
    "ingest(r'pdf\\manual_atencao_mulher_climaterio.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8292c620",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading weights: 100%|██████████| 100/100 [00:00<00:00, 422.17it/s, Materializing param=shared.weight]                                                     \n",
      "The tied weights mapping and config for this model specifies to tie shared.weight to encoder.embed_tokens.weight, but both are present in the checkpoints, so we will NOT tie them. You should update the config with `tie_word_embeddings=False` to silence this warning\n"
     ]
    }
   ],
   "source": [
    "ingest(r'pdf\\manual_suplementacao_ferro_condutas_gerais.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7d9d7c61",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading weights: 100%|██████████| 100/100 [00:00<00:00, 534.45it/s, Materializing param=shared.weight]                                                      \n",
      "The tied weights mapping and config for this model specifies to tie shared.weight to encoder.embed_tokens.weight, but both are present in the checkpoints, so we will NOT tie them. You should update the config with `tie_word_embeddings=False` to silence this warning\n"
     ]
    }
   ],
   "source": [
    "ingest(r'pdf\\pcdt_endometriose_2016-1.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cd13c804",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading weights: 100%|██████████| 100/100 [00:00<00:00, 487.73it/s, Materializing param=shared.weight]                                                     \n",
      "The tied weights mapping and config for this model specifies to tie shared.weight to encoder.embed_tokens.weight, but both are present in the checkpoints, so we will NOT tie them. You should update the config with `tie_word_embeddings=False` to silence this warning\n"
     ]
    }
   ],
   "source": [
    "ingest(r'pdf\\saude_sexual_saude_reprodutiva.pdf')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11dc11b2",
   "metadata": {},
   "source": [
    "## Memória PostgreSQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "636de341",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sqlalchemy import create_engine, text\n",
    "from sqlalchemy.orm import sessionmaker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9e52646d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# agent/memory.py\n",
    "\n",
    "# Engine (pool gerenciado automaticamente)\n",
    "engine = create_engine(\n",
    "    \"postgresql+psycopg://postgres:postgres@127.0.0.1:5433/postgres\",\n",
    "    pool_pre_ping=True\n",
    ")\n",
    "\n",
    "SessionLocal = sessionmaker(bind=engine)\n",
    "\n",
    "def save_memory(user_id: str, role: str, content: str):\n",
    "    with SessionLocal() as session:\n",
    "        session.execute(\n",
    "            text(\"\"\"\n",
    "                INSERT INTO memory (user_id, role, content)\n",
    "                VALUES (:user_id, :role, :content)\n",
    "            \"\"\"),\n",
    "            {\n",
    "                \"user_id\": user_id,\n",
    "                \"role\": role,\n",
    "                \"content\": content\n",
    "            }\n",
    "        )\n",
    "        session.commit()\n",
    "\n",
    "def load_memory(user_id: str):\n",
    "    with SessionLocal() as session:\n",
    "        result = session.execute(\n",
    "            text(\"\"\"\n",
    "                SELECT role, content\n",
    "                FROM memory\n",
    "                WHERE user_id = :user_id\n",
    "                ORDER BY created_at\n",
    "            \"\"\"),\n",
    "            {\"user_id\": user_id}\n",
    "        )\n",
    "        return result.fetchall()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed794122",
   "metadata": {},
   "source": [
    "## Tool de RAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f22347e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_chroma import Chroma\n",
    "from langchain_huggingface import HuggingFaceEmbeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e4ce3f23",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading weights: 100%|██████████| 100/100 [00:00<00:00, 449.32it/s, Materializing param=shared.weight]                                                     \n",
      "The tied weights mapping and config for this model specifies to tie shared.weight to encoder.embed_tokens.weight, but both are present in the checkpoints, so we will NOT tie them. You should update the config with `tie_word_embeddings=False` to silence this warning\n",
      "Exception in thread Thread-auto_conversion:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\erico\\Documents\\Bertha-Lutz-AI\\env\\Lib\\site-packages\\httpx\\_transports\\default.py\", line 101, in map_httpcore_exceptions\n",
      "    yield\n",
      "  File \"c:\\Users\\erico\\Documents\\Bertha-Lutz-AI\\env\\Lib\\site-packages\\httpx\\_transports\\default.py\", line 250, in handle_request\n",
      "    resp = self._pool.handle_request(req)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\erico\\Documents\\Bertha-Lutz-AI\\env\\Lib\\site-packages\\httpcore\\_sync\\connection_pool.py\", line 256, in handle_request\n",
      "    raise exc from None\n",
      "  File \"c:\\Users\\erico\\Documents\\Bertha-Lutz-AI\\env\\Lib\\site-packages\\httpcore\\_sync\\connection_pool.py\", line 236, in handle_request\n",
      "    response = connection.handle_request(\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\erico\\Documents\\Bertha-Lutz-AI\\env\\Lib\\site-packages\\httpcore\\_sync\\connection.py\", line 103, in handle_request\n",
      "    return self._connection.handle_request(request)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\erico\\Documents\\Bertha-Lutz-AI\\env\\Lib\\site-packages\\httpcore\\_sync\\http11.py\", line 136, in handle_request\n",
      "    raise exc\n",
      "  File \"c:\\Users\\erico\\Documents\\Bertha-Lutz-AI\\env\\Lib\\site-packages\\httpcore\\_sync\\http11.py\", line 106, in handle_request\n",
      "    ) = self._receive_response_headers(**kwargs)\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\erico\\Documents\\Bertha-Lutz-AI\\env\\Lib\\site-packages\\httpcore\\_sync\\http11.py\", line 177, in _receive_response_headers\n",
      "    event = self._receive_event(timeout=timeout)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\erico\\Documents\\Bertha-Lutz-AI\\env\\Lib\\site-packages\\httpcore\\_sync\\http11.py\", line 217, in _receive_event\n",
      "    data = self._network_stream.read(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\erico\\Documents\\Bertha-Lutz-AI\\env\\Lib\\site-packages\\httpcore\\_backends\\sync.py\", line 126, in read\n",
      "    with map_exceptions(exc_map):\n",
      "  File \"C:\\Users\\erico\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\contextlib.py\", line 155, in __exit__\n",
      "    self.gen.throw(typ, value, traceback)\n",
      "  File \"c:\\Users\\erico\\Documents\\Bertha-Lutz-AI\\env\\Lib\\site-packages\\httpcore\\_exceptions.py\", line 14, in map_exceptions\n",
      "    raise to_exc(exc) from exc\n",
      "httpcore.ReadTimeout: The read operation timed out\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\erico\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\threading.py\", line 1038, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"C:\\Users\\erico\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\threading.py\", line 975, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"c:\\Users\\erico\\Documents\\Bertha-Lutz-AI\\env\\Lib\\site-packages\\transformers\\safetensors_conversion.py\", line 117, in auto_conversion\n",
      "    raise e\n",
      "  File \"c:\\Users\\erico\\Documents\\Bertha-Lutz-AI\\env\\Lib\\site-packages\\transformers\\safetensors_conversion.py\", line 96, in auto_conversion\n",
      "    sha = get_conversion_pr_reference(api, pretrained_model_name_or_path, **cached_file_kwargs)\n",
      "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\erico\\Documents\\Bertha-Lutz-AI\\env\\Lib\\site-packages\\transformers\\safetensors_conversion.py\", line 69, in get_conversion_pr_reference\n",
      "    pr = previous_pr(api, model_id, pr_title, token=token)\n",
      "         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\erico\\Documents\\Bertha-Lutz-AI\\env\\Lib\\site-packages\\transformers\\safetensors_conversion.py\", line 14, in previous_pr\n",
      "    for discussion in get_repo_discussions(repo_id=model_id, token=token):\n",
      "  File \"c:\\Users\\erico\\Documents\\Bertha-Lutz-AI\\env\\Lib\\site-packages\\huggingface_hub\\hf_api.py\", line 6350, in get_repo_discussions\n",
      "    discussions, has_next = _fetch_discussion_page(page_index=page_index)\n",
      "                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\erico\\Documents\\Bertha-Lutz-AI\\env\\Lib\\site-packages\\huggingface_hub\\hf_api.py\", line 6338, in _fetch_discussion_page\n",
      "    resp = get_session().get(path, headers=headers, params=params)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\erico\\Documents\\Bertha-Lutz-AI\\env\\Lib\\site-packages\\httpx\\_client.py\", line 1053, in get\n",
      "    return self.request(\n",
      "           ^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\erico\\Documents\\Bertha-Lutz-AI\\env\\Lib\\site-packages\\httpx\\_client.py\", line 825, in request\n",
      "    return self.send(request, auth=auth, follow_redirects=follow_redirects)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\erico\\Documents\\Bertha-Lutz-AI\\env\\Lib\\site-packages\\httpx\\_client.py\", line 914, in send\n",
      "    response = self._send_handling_auth(\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\erico\\Documents\\Bertha-Lutz-AI\\env\\Lib\\site-packages\\httpx\\_client.py\", line 942, in _send_handling_auth\n",
      "    response = self._send_handling_redirects(\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\erico\\Documents\\Bertha-Lutz-AI\\env\\Lib\\site-packages\\httpx\\_client.py\", line 979, in _send_handling_redirects\n",
      "    response = self._send_single_request(request)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\erico\\Documents\\Bertha-Lutz-AI\\env\\Lib\\site-packages\\httpx\\_client.py\", line 1014, in _send_single_request\n",
      "    response = transport.handle_request(request)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\erico\\Documents\\Bertha-Lutz-AI\\env\\Lib\\site-packages\\httpx\\_transports\\default.py\", line 249, in handle_request\n",
      "    with map_httpcore_exceptions():\n",
      "  File \"C:\\Users\\erico\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\contextlib.py\", line 155, in __exit__\n",
      "    self.gen.throw(typ, value, traceback)\n",
      "  File \"c:\\Users\\erico\\Documents\\Bertha-Lutz-AI\\env\\Lib\\site-packages\\httpx\\_transports\\default.py\", line 118, in map_httpcore_exceptions\n",
      "    raise mapped_exc(message) from exc\n",
      "httpx.ReadTimeout: The read operation timed out\n",
      "C:\\Users\\erico\\AppData\\Local\\Temp\\ipykernel_6460\\3269479901.py:4: LangChainDeprecationWarning: The class `Chroma` was deprecated in LangChain 0.2.9 and will be removed in 1.0. An updated version of the class exists in the `langchain-chroma package and should be used instead. To use it run `pip install -U `langchain-chroma` and import as `from `langchain_chroma import Chroma``.\n",
      "  vectordb = Chroma(persist_directory=\"chroma_db\", embedding_function=embeddings)\n"
     ]
    }
   ],
   "source": [
    "# agent/tools.py\n",
    "\n",
    "embeddings = HuggingFaceEmbeddings(model_name=\"hkunlp/instructor-base\")\n",
    "vectordb = Chroma(persist_directory=\"chroma_db\", embedding_function=embeddings)\n",
    "\n",
    "def buscar_protocolo(query: str):\n",
    "    docs = vectordb.similarity_search(query, k=3)\n",
    "    return \"\\n\".join([d.page_content for d in docs])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "615f9d43",
   "metadata": {},
   "source": [
    "## Guardrails médicos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a4fcf439",
   "metadata": {},
   "outputs": [],
   "source": [
    "# agent/guardrails.py\n",
    "\n",
    "def aplicar_guardrails(resposta: str):\n",
    "    proibidos = [\"diagnóstico\", \"tome\", \"medicamento\", \"dosagem\"]\n",
    "\n",
    "    for p in proibidos:\n",
    "        if p in resposta.lower():\n",
    "            return (\n",
    "                \"Não posso fornecer diagnóstico ou prescrição. \"\n",
    "                \"Recomendo procurar uma UBS para avaliação profissional.\\n\\n\"\n",
    "                + resposta\n",
    "            )\n",
    "    return resposta"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4db86f5f",
   "metadata": {},
   "source": [
    "## LangGraph (o cérebro)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45b3f9c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# agent/graph.py\n",
    "import os\n",
    "from langgraph.graph import StateGraph\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.messages import SystemMessage, HumanMessage\n",
    "from typing import TypedDict\n",
    "# from agent.tools import buscar_protocolo\n",
    "# from agent.guardrails import aplicar_guardrails"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "d782cfc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ.get(\"OPENAI_API_KEY\")\n",
    "\n",
    "llm = ChatOpenAI(model=\"gpt-4o-mini\", max_tokens=300, temperature=0.5) # pode trocar por Ollama depois"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "d2d750f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AgentState(TypedDict):\n",
    "    input: str\n",
    "    contexto: str\n",
    "    resposta: str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "f0273b21",
   "metadata": {},
   "outputs": [],
   "source": [
    "def node_rag(state):\n",
    "    contexto = buscar_protocolo(state[\"input\"])\n",
    "    state[\"contexto\"] = contexto\n",
    "    return state\n",
    "\n",
    "def node_llm(state):\n",
    "    messages = [\n",
    "        SystemMessage(\n",
    "            content=\"Você é um agente especializado em saúde da mulher, baseado em diretrizes oficiais.\"\n",
    "        ),\n",
    "        HumanMessage(\n",
    "            content=f\"\"\"\n",
    "Contexto oficial:\n",
    "{state['contexto']}\n",
    "\n",
    "Pergunta da paciente:\n",
    "{state['input']}\n",
    "\"\"\"\n",
    "        )\n",
    "    ]\n",
    "\n",
    "    resposta = llm.invoke(messages).content\n",
    "    state[\"resposta\"] = aplicar_guardrails(resposta)\n",
    "    return state\n",
    "\n",
    "def node_guardrails(state):\n",
    "    return state\n",
    "\n",
    "graph = StateGraph(AgentState)\n",
    "graph.add_node(\"rag\", node_rag)\n",
    "graph.add_node(\"llm\", node_llm)\n",
    "graph.add_node(\"guardrails\", node_guardrails)\n",
    "\n",
    "graph.set_entry_point(\"rag\")\n",
    "\n",
    "graph.add_edge(\"rag\", \"llm\")\n",
    "graph.add_edge(\"llm\", \"guardrails\")\n",
    "\n",
    "app = graph.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "4b0539ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---\n",
      "config:\n",
      "  flowchart:\n",
      "    curve: linear\n",
      "---\n",
      "graph TD;\n",
      "\t__start__([<p>__start__</p>]):::first\n",
      "\trag(rag)\n",
      "\tllm(llm)\n",
      "\tguardrails(guardrails)\n",
      "\t__end__([<p>__end__</p>]):::last\n",
      "\t__start__ --> rag;\n",
      "\tllm --> guardrails;\n",
      "\trag --> llm;\n",
      "\tguardrails --> __end__;\n",
      "\tclassDef default fill:#f2f0ff,line-height:1.2\n",
      "\tclassDef first fill-opacity:0\n",
      "\tclassDef last fill:#bfb6fc\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(app.get_graph().draw_mermaid())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9de443df",
   "metadata": {},
   "source": [
    "## Execução"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "2911ed3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# main.py\n",
    "# from agent.graph import app\n",
    "# from agent.memory import save_memory, load_memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "bcf14d81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# USER_ID = \"paciente_001\"\n",
    "\n",
    "# while True:\n",
    "#     pergunta = input(\"Paciente: \")\n",
    "\n",
    "#     historico = load_memory(USER_ID)\n",
    "\n",
    "#     result = app.invoke({\n",
    "#         \"input\": pergunta,\n",
    "#         \"history\": historico\n",
    "#     })\n",
    "\n",
    "#     resposta = result[\"resposta\"]\n",
    "\n",
    "#     print(\"\\nAgente:\", resposta, \"\\n\")\n",
    "\n",
    "#     save_memory(USER_ID, \"user\", pergunta)\n",
    "#     save_memory(USER_ID, \"agent\", resposta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6fe77f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "USER_ID = \"paciente_002\"\n",
    "\n",
    "def conversar(pergunta: str):\n",
    "    historico = load_memory(USER_ID)\n",
    "\n",
    "    result = app.invoke({\n",
    "        \"input\": pergunta,\n",
    "        \"history\": historico\n",
    "    })\n",
    "\n",
    "    resposta = result[\"resposta\"]\n",
    "    print(resposta)\n",
    "    save_memory(USER_ID, \"user\", pergunta)\n",
    "    save_memory(USER_ID, \"agent\", resposta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "b6ad75c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dor pélvica forte e sangramento intenso não são considerados normais e podem indicar uma condição médica que requer avaliação imediata. É importante que você procure um profissional de saúde para uma avaliação completa. \n",
      "\n",
      "Possíveis causas para esses sintomas podem incluir:\n",
      "\n",
      "- Problemas ginecológicos, como miomas, endometriose ou cistos ovarianos.\n",
      "- Complicações em uma gravidez, como aborto espontâneo ou gravidez ectópica.\n",
      "- Infecções, como doença inflamatória pélvica.\n",
      "- Alterações hormonais.\n",
      "\n",
      "A avaliação médica pode incluir um exame físico, ultrassonografia e, se necessário, outros exames laboratoriais para determinar a causa do seu sangramento e dor. Não hesite em buscar ajuda, especialmente se os sintomas forem intensos ou acompanhados de outros sinais, como febre ou tontura.\n"
     ]
    }
   ],
   "source": [
    "conversar(\"Estou com dor pélvica forte e muito sangramento. Isso é normal?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "98e86333",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Não posso fornecer diagnóstico ou prescrição. Recomendo procurar uma UBS para avaliação profissional.\n",
      "\n",
      "É importante que você realize o exame preventivo, conhecido como Papanicolau (ou citologia cervical), que é fundamental para a detecção precoce de alterações nas células do colo do útero, incluindo câncer cervical. Embora a recomendação geral para o rastreamento do câncer cervical inicie aos 25 anos, é aconselhável que você faça seu primeiro exame o quanto antes, principalmente se você tem vida sexual ativa.\n",
      "\n",
      "Além do Papanicolau, é essencial que você também faça o rastreamento para infecções sexualmente transmissíveis (ISTs), como a sífilis, especialmente considerando que a faixa etária de 13 a 29 anos tem mostrado um aumento nas notificações de sífilis. \n",
      "\n",
      "Recomendo que você procure uma unidade de saúde para agendar seu exame preventivo e discutir quaisquer outras preocupações relacionadas à sua saúde sexual. A prevenção e o diagnóstico precoce são fundamentais para garantir sua saúde e bem-estar.\n"
     ]
    }
   ],
   "source": [
    "conversar(\"Tenho 32 anos e nunca fiz preventivo.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
