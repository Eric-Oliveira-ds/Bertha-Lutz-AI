{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a949802a",
   "metadata": {},
   "source": [
    "# BERTHA LUTZ DEV AGENTIC"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6c278d1",
   "metadata": {},
   "source": [
    "- LLM responde usando RAG dos protocolos do MinistÃ©rio da SaÃºde > Guarda histÃ³rico no PostgreSQL > Aplica guardrails (nÃ£o diagnostica, nÃ£o prescreve)> Usa LangGraph + tools"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6f08cbd",
   "metadata": {},
   "source": [
    "## Ingerir PDFs do MS no Chroma (RAG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "df5b233d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\erico\\Documents\\Bertha-Lutz-AI\\env\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_huggingface import HuggingFaceEmbeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "41cc0b19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# rag/ingest.py\n",
    "\n",
    "def ingest(pdf_path: str):\n",
    "    loader = PyPDFLoader(pdf_path)\n",
    "    docs = loader.load()\n",
    "\n",
    "    splitter = RecursiveCharacterTextSplitter(chunk_size=800, chunk_overlap=100)\n",
    "    chunks = splitter.split_documents(docs)\n",
    "\n",
    "    embeddings = HuggingFaceEmbeddings(model_name=\"hkunlp/instructor-base\")\n",
    "\n",
    "    vectordb = Chroma.from_documents(\n",
    "        chunks,\n",
    "        embedding=embeddings,\n",
    "        persist_directory=\"chroma_db\"\n",
    "    )\n",
    "\n",
    "    vectordb.persist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b8a26c2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading weights: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:00<00:00, 331.85it/s, Materializing param=shared.weight]                                                     \n",
      "The tied weights mapping and config for this model specifies to tie shared.weight to encoder.embed_tokens.weight, but both are present in the checkpoints, so we will NOT tie them. You should update the config with `tie_word_embeddings=False` to silence this warning\n",
      "C:\\Users\\erico\\AppData\\Local\\Temp\\ipykernel_18816\\3887741603.py:18: LangChainDeprecationWarning: Since Chroma 0.4.x the manual persistence method is no longer supported as docs are automatically persisted.\n",
      "  vectordb.persist()\n"
     ]
    }
   ],
   "source": [
    "ingest(r'pdf\\Consensointegra.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b1d3ff35",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading weights: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:00<00:00, 421.94it/s, Materializing param=shared.weight]                                                     \n",
      "The tied weights mapping and config for this model specifies to tie shared.weight to encoder.embed_tokens.weight, but both are present in the checkpoints, so we will NOT tie them. You should update the config with `tie_word_embeddings=False` to silence this warning\n"
     ]
    }
   ],
   "source": [
    "ingest(r'pdf\\relatorio-preliminar-diretrizes-brasileiras-para-o-rastreamento-do-cancer-do-colo-do-utero-parte-i-rastreamento-organizado-utilizando-testes-moleculares-para-deteccao-de-dna-hpv-oncogenico.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4011b225",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading weights: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:00<00:00, 338.49it/s, Materializing param=shared.weight]                                                     \n",
      "The tied weights mapping and config for this model specifies to tie shared.weight to encoder.embed_tokens.weight, but both are present in the checkpoints, so we will NOT tie them. You should update the config with `tie_word_embeddings=False` to silence this warning\n"
     ]
    }
   ],
   "source": [
    "ingest(r'pdf\\Manual da Gestante.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e28fd7f5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4bdc4ef",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6f86d52",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "11dc11b2",
   "metadata": {},
   "source": [
    "## MemÃ³ria PostgreSQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "636de341",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sqlalchemy import create_engine, text\n",
    "from sqlalchemy.orm import sessionmaker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9e52646d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# agent/memory.py\n",
    "\n",
    "# Engine (pool gerenciado automaticamente)\n",
    "engine = create_engine(\n",
    "    \"postgresql+psycopg://postgres:postgres@127.0.0.1:5433/postgres\",\n",
    "    pool_pre_ping=True\n",
    ")\n",
    "\n",
    "SessionLocal = sessionmaker(bind=engine)\n",
    "\n",
    "def save_memory(user_id: str, role: str, content: str):\n",
    "    with SessionLocal() as session:\n",
    "        session.execute(\n",
    "            text(\"\"\"\n",
    "                INSERT INTO memory (user_id, role, content)\n",
    "                VALUES (:user_id, :role, :content)\n",
    "            \"\"\"),\n",
    "            {\n",
    "                \"user_id\": user_id,\n",
    "                \"role\": role,\n",
    "                \"content\": content\n",
    "            }\n",
    "        )\n",
    "        session.commit()\n",
    "\n",
    "def load_memory(user_id: str):\n",
    "    with SessionLocal() as session:\n",
    "        result = session.execute(\n",
    "            text(\"\"\"\n",
    "                SELECT role, content\n",
    "                FROM memory\n",
    "                WHERE user_id = :user_id\n",
    "                ORDER BY created_at\n",
    "            \"\"\"),\n",
    "            {\"user_id\": user_id}\n",
    "        )\n",
    "        return result.fetchall()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed794122",
   "metadata": {},
   "source": [
    "## Tool de RAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f22347e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_huggingface import HuggingFaceEmbeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e4ce3f23",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading weights: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:00<00:00, 469.96it/s, Materializing param=shared.weight]                                                     \n",
      "The tied weights mapping and config for this model specifies to tie shared.weight to encoder.embed_tokens.weight, but both are present in the checkpoints, so we will NOT tie them. You should update the config with `tie_word_embeddings=False` to silence this warning\n",
      "Warning: You are sending unauthenticated requests to the HF Hub. Please set a HF_TOKEN to enable higher rate limits and faster downloads.\n",
      "C:\\Users\\erico\\AppData\\Local\\Temp\\ipykernel_18336\\3269479901.py:4: LangChainDeprecationWarning: The class `Chroma` was deprecated in LangChain 0.2.9 and will be removed in 1.0. An updated version of the class exists in the `langchain-chroma package and should be used instead. To use it run `pip install -U `langchain-chroma` and import as `from `langchain_chroma import Chroma``.\n",
      "  vectordb = Chroma(persist_directory=\"chroma_db\", embedding_function=embeddings)\n"
     ]
    }
   ],
   "source": [
    "# agent/tools.py\n",
    "\n",
    "embeddings = HuggingFaceEmbeddings(model_name=\"hkunlp/instructor-base\")\n",
    "vectordb = Chroma(persist_directory=\"chroma_db\", embedding_function=embeddings)\n",
    "\n",
    "def buscar_protocolo(query: str):\n",
    "    docs = vectordb.similarity_search(query, k=3)\n",
    "    return \"\\n\".join([d.page_content for d in docs])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "615f9d43",
   "metadata": {},
   "source": [
    "## Guardrails mÃ©dicos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a4fcf439",
   "metadata": {},
   "outputs": [],
   "source": [
    "# agent/guardrails.py\n",
    "\n",
    "def aplicar_guardrails(resposta: str):\n",
    "    proibidos = [\"diagnÃ³stico\", \"tome\", \"medicamento\", \"dosagem\"]\n",
    "\n",
    "    for p in proibidos:\n",
    "        if p in resposta.lower():\n",
    "            return (\n",
    "                \"NÃ£o posso fornecer diagnÃ³stico ou prescriÃ§Ã£o. \"\n",
    "                \"Recomendo procurar uma UBS para avaliaÃ§Ã£o profissional.\\n\\n\"\n",
    "                + resposta\n",
    "            )\n",
    "    return resposta"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4db86f5f",
   "metadata": {},
   "source": [
    "## LangGraph (o cÃ©rebro)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "45b3f9c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# agent/graph.py\n",
    "import os\n",
    "from langgraph.graph import StateGraph\n",
    "from langchain.chat_models import init_chat_model\n",
    "# from agent.tools import buscar_protocolo\n",
    "# from agent.guardrails import aplicar_guardrails"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d782cfc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ.get(\"OPENAI_API_KEY\")\n",
    "\n",
    "llm = init_chat_model(\"gpt-4.1\") # pode trocar por Ollama depois"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f0273b21",
   "metadata": {},
   "outputs": [],
   "source": [
    "def node_rag(state):\n",
    "    contexto = buscar_protocolo(state[\"input\"])\n",
    "    state[\"contexto\"] = contexto\n",
    "    return state\n",
    "\n",
    "def node_llm(state):\n",
    "    prompt = f\"\"\"\n",
    "VocÃª Ã© um agente de saÃºde da mulher.\n",
    "\n",
    "Contexto oficial:\n",
    "{state['contexto']}\n",
    "\n",
    "Pergunta da paciente:\n",
    "{state['input']}\n",
    "\"\"\"\n",
    "    resposta = llm.invoke(prompt).content\n",
    "    state[\"resposta\"] = aplicar_guardrails(resposta)\n",
    "    return state\n",
    "\n",
    "graph = StateGraph(dict)\n",
    "graph.add_node(\"rag\", node_rag)\n",
    "graph.add_node(\"llm\", node_llm)\n",
    "\n",
    "graph.set_entry_point(\"rag\")\n",
    "graph.add_edge(\"rag\", \"llm\")\n",
    "\n",
    "app = graph.compile()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9de443df",
   "metadata": {},
   "source": [
    "## ExecuÃ§Ã£o"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2911ed3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# main.py\n",
    "# from agent.graph import app\n",
    "# from agent.memory import save_memory, load_memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "bcf14d81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# USER_ID = \"paciente_001\"\n",
    "\n",
    "# while True:\n",
    "#     pergunta = input(\"Paciente: \")\n",
    "\n",
    "#     historico = load_memory(USER_ID)\n",
    "\n",
    "#     result = app.invoke({\n",
    "#         \"input\": pergunta,\n",
    "#         \"history\": historico\n",
    "#     })\n",
    "\n",
    "#     resposta = result[\"resposta\"]\n",
    "\n",
    "#     print(\"\\nAgente:\", resposta, \"\\n\")\n",
    "\n",
    "#     save_memory(USER_ID, \"user\", pergunta)\n",
    "#     save_memory(USER_ID, \"agent\", resposta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e6fe77f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "USER_ID = \"paciente_001\"\n",
    "\n",
    "def conversar(pergunta: str):\n",
    "    historico = load_memory(USER_ID)\n",
    "\n",
    "    result = app.invoke({\n",
    "        \"input\": pergunta,\n",
    "        \"history\": historico\n",
    "    })\n",
    "\n",
    "    resposta = result[\"resposta\"]\n",
    "\n",
    "    print(\"Agente:\\n\")\n",
    "    print(resposta)\n",
    "    print(\"\\n\" + \"-\"*60 + \"\\n\")\n",
    "\n",
    "    save_memory(USER_ID, \"user\", pergunta)\n",
    "    save_memory(USER_ID, \"agent\", resposta)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b6ad75c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Agente:\n",
      "\n",
      "OlÃ¡! Recebi sua pergunta e entendo sua preocupaÃ§Ã£o.\n",
      "\n",
      "Dor pÃ©lvica forte e muito sangramento **nÃ£o sÃ£o sinais normais** durante a gestaÃ§Ã£o. Apesar de um pequeno sangramento leve (Ã s vezes rosado ou marrom) poder ocorrer em algumas etapas da gravidez, principalmente no inÃ­cio, **sangramento intenso** e **dor forte** juntos podem indicar uma situaÃ§Ã£o mais sÃ©ria que **necessita de avaliaÃ§Ã£o mÃ©dica imediata**.\n",
      "\n",
      "Esses sintomas podem estar associados a situaÃ§Ãµes que precisam de cuidados, como:\n",
      "- Descolamento ovular ou descolamento de placenta;\n",
      "- AmeaÃ§a de aborto ou aborto em curso;\n",
      "- Gravidez ectÃ³pica (fora do Ãºtero), especialmente se a dor estiver localizada de um lado;\n",
      "- Outras causas ginecolÃ³gicas ou obstÃ©tricas, que sÃ³ podem ser identificadas com avaliaÃ§Ã£o presencial.\n",
      "\n",
      "**OrientaÃ§Ã£o:**  \n",
      "- Procure imediatamente uma maternidade ou pronto atendimento prÃ³ximo para ser avaliada.\n",
      "- Leve consigo seus documentos e, se possÃ­vel, um resumo do seu prÃ©-natal.\n",
      "- NÃ£o espere o sangramento ou a dor passarem sozinhos antes de buscar ajuda.\n",
      "\n",
      "**Lembre-se:** Na gestaÃ§Ã£o, Ã© sempre melhor pecar pelo excesso de cuidado! Estou Ã  disposiÃ§Ã£o para ajudar, mas nesse momento, a avaliaÃ§Ã£o presencial Ã© fundamental.\n",
      "\n",
      "**Se sentir fraqueza intensa, palidez, tontura, suor frio, febre ou queda da pressÃ£o, procure socorro imediatamente.**\n",
      "\n",
      "------------------------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "conversar(\"Estou com dor pÃ©lvica forte e muito sangramento. Isso Ã© normal?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "98e86333",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Agente:\n",
      "\n",
      "OlÃ¡! Vou te orientar de acordo com as recomendaÃ§Ãµes oficiais e base cientÃ­fica mais atual.\n",
      "\n",
      "VocÃª tem **32 anos** e **nunca fez o exame preventivo** (Papanicolau/citopatolÃ³gico) para cÃ¢ncer do colo do Ãºtero. Ã‰ muito importante que vocÃª **faÃ§a o rastreamento o quanto antes**!\n",
      "\n",
      "### O que diz o protocolo oficial?\n",
      "- **Mulheres entre 25 e 64 anos** devem realizar o rastreamento para cÃ¢ncer do colo do Ãºtero.\n",
      "- Mulheres de **25 a 29 anos que nunca fizeram o rastreamento** tÃªm prioridade, mas aos **32 anos** vocÃª ainda estÃ¡ dentro da faixa recomendada e hÃ¡ muita vantagem em iniciar agora.\n",
      "- Para iniciar, vocÃª deve realizar o exame de **citologia (Papanicolau)** ou, se disponÃ­vel em sua regiÃ£o, o **teste de DNA-HPV** oncogÃªnico, que demonstra maior eficÃ¡cia na detecÃ§Ã£o precoce do cÃ¢ncer e suas lesÃµes iniciais.\n",
      "- O rastreamento deve ser realizado a cada **3 anos** apÃ³s dois exames anuais consecutivos normais. Se disponÃ­vel, o teste de DNA-HPV pode ter intervalo maior entre exames, se o resultado for negativo.\n",
      "\n",
      "### Por quÃª Ã© importante iniciar agora?\n",
      "- O rastreamento detecta **alteraÃ§Ãµes precoces** antes do desenvolvimento do cÃ¢ncer.\n",
      "- O cÃ¢ncer do colo do Ãºtero tem maior prevenÃ§Ã£o e cura quando diagnosticado precocemente.\n",
      "- O grupo prioritÃ¡rio para a OMS Ã© 30 a 49 anos.\n",
      "\n",
      "### Onde fazer?\n",
      "Procure a unidade de saÃºde mais prÃ³xima (posto de saÃºde/UBS/ESF) e informe que **nunca realizou o exame**. Isso te coloca como prioridade para agendamento.\n",
      "\n",
      "---\n",
      "\n",
      "#### Resumindo:\n",
      "- **VocÃª deve agendar imediatamente** o exame, pois estÃ¡ dentro da faixa etÃ¡ria e tem indicaÃ§Ã£o clara.\n",
      "- Se possÃ­vel, peÃ§a informaÃ§Ãµes sobre a possibilidade de fazer o exame de **DNA-HPV**.\n",
      "- Mantenha o acompanhamento regular conforme orientaÃ§Ã£o profissional.\n",
      "\n",
      "Se tiver dÃºvidas sobre o exame, preparo ou resultados, pode perguntar! ParabÃ©ns pela preocupaÃ§Ã£o com a saÃºde! ðŸ’™\n",
      "\n",
      "------------------------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "conversar(\"Tenho 32 anos e nunca fiz preventivo.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
